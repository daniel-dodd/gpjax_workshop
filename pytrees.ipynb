{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93d0e0a",
   "metadata": {},
   "source": [
    "# PyTree's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca0136",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/daniel-dodd/gpjax_workshop/blob/main/pytrees.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8feb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpjax==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396398e8",
   "metadata": {},
   "source": [
    "`GPJax`Â **represents all objects as JAX\n",
    "[_PyTrees_](https://jax.readthedocs.io/en/latest/pytrees.html)**, giving\n",
    "\n",
    "- A simple API with a **TensorFlow / PyTorch feel** ...\n",
    "- ... whilst **fully compatible** with JAX's functional paradigm ...\n",
    "- ... And **works out of the box** (no filtering) with JAX's transformations\n",
    "  such as `grad`.\n",
    "\n",
    "We achieve this through providing a base `Module` abstraction to cleanly\n",
    "handle parameter trainability and optimising transformations of JAX models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ff3b7",
   "metadata": {},
   "source": [
    "\n",
    "### The RBF kernel\n",
    "\n",
    "\n",
    "Intuitively, the kernel defines the notion of *similarity* between\n",
    "the value of the function at two points, $f(\\mathbf{x})$ and $f(\\mathbf{x}')$, and\n",
    "will be denoted as $k(\\mathbf{x}, \\mathbf{x}')$:\n",
    "\n",
    "$$\\begin{aligned} k(\\mathbf{x}, \\mathbf{x}') &= \\text{Cov}[f(\\mathbf{x}),\n",
    "f(\\mathbf{x}')] \\end{aligned}$$\n",
    "\n",
    "For the RBF kernel, we have\n",
    "\n",
    "$$\n",
    "k(x, y) = \\sigma^2\\exp\\left(\\frac{\\lVert\n",
    "x-y\\rVert_{2}^2}{2\\ell^2} \\right)\n",
    "$$\n",
    "\n",
    "- $\\sigma^2\\in\\mathbb{R}_{>0}$ is a\n",
    "variance parameter \n",
    "-  $\\ell^2\\in\\mathbb{R}_{>0}$ a lengthscale parameter.\n",
    "\n",
    "Terming the evaluation of $`k(x, y)`$ the _covariance_, we can represent\n",
    "this object as a Python `dataclass` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74185416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RBF:\n",
    "    lengthscale: float = field(default=1.0)\n",
    "    variance: float = field(default=1.0)\n",
    "\n",
    "    def __call__(self, x: float, y: float) -> jax.Array:\n",
    "        return self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab534f0",
   "metadata": {},
   "source": [
    "Here, the Python `dataclass` is a class that simplifies the process of\n",
    "creating classes that primarily store data. It reduces boilerplate code and\n",
    "provides convenient methods for initialising and representing the data. An\n",
    "equivalent class could be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780e1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, lengthscale: float = 1.0, variance: float = 1.0) -> None:\n",
    "        self.lengthscale = lengthscale\n",
    "        self.variance = variance\n",
    "\n",
    "    def __call__(self, x: float, y: float) -> jax.Array:\n",
    "        return self.variance * jnp.exp(-0.5 * ((x-y) / self.lengthscale)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03949f",
   "metadata": {},
   "source": [
    "To establish some terminology, within the above RBF `dataclass`, we refer to\n",
    "the lengthscale and variance as _fields_. Further, the `RBF.__call__` is a\n",
    "_method_. So far so good. However, if we wanted to take the gradient of\n",
    "the kernel with respect to its parameters $`\\nabla_{\\ell, \\sigma^2} k(1.0, 2.0;\n",
    "\\ell, \\sigma^2)`$ at inputs $`x=1.0`$ and $`y=2.0`$, then we encounter a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df0aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument '<__main__.RBF object at 0x293b51b40>' of type <class '__main__.RBF'> is not a valid JAX type.\n"
     ]
    }
   ],
   "source": [
    "kernel = RBF()\n",
    "\n",
    "try:\n",
    "    jax.grad(lambda kern: kern(1.0, 2.0))(kernel)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a29b48",
   "metadata": {},
   "source": [
    "This issues arises as the object we have defined is not yet\n",
    "compatible with JAX. To achieve this we must consider [JAX's _PyTree_](https://jax.readthedocs.io/en/latest/pytrees.html)\n",
    "abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1ce22",
   "metadata": {},
   "source": [
    "### PyTrees\n",
    "\n",
    "JAX PyTrees are a powerful tool in the JAX library that enable users to work\n",
    "with complex data structures in a way that is efficient, flexible, and easy to\n",
    "use. A PyTree is a data structure that is composed of other data\n",
    "structures, and it can be thought of as a tree where each 'node' is either a\n",
    "leaf (a simple data structure) or another PyTree. By default, the set\n",
    "of 'node' types that are regarded a PyTree are Python lists, tuples, and\n",
    "dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06ca192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45492e08",
   "metadata": {},
   "source": [
    "#### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61e598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.14, {'Monte': <object object at 0x293d95270>, 'Carlo': False}]\n"
     ]
    }
   ],
   "source": [
    "tree = [3.14, {\"Monte\": object(), \"Carlo\": False}]\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc435e",
   "metadata": {},
   "source": [
    "is a PyTree with structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e2bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef([*, {'Carlo': *, 'Monte': *}])\n"
     ]
    }
   ],
   "source": [
    "print(jtu.tree_structure(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6b1e1",
   "metadata": {},
   "source": [
    "with the following leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509eb806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.14, False, <object object at 0x293d95270>]\n"
     ]
    }
   ],
   "source": [
    "print(jtu.tree_leaves(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45b5c5",
   "metadata": {},
   "source": [
    "#### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b32b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = (\n",
    "    jnp.array([1.0, 2.0, 3.0]),\n",
    "    jnp.array([4.0, 5.0, 6.0]),\n",
    "    jnp.array([7.0, 8.0, 9.0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d42d63",
   "metadata": {},
   "source": [
    "You can use this template to perform various operations on the data, such as\n",
    "applying a function to each leaf of the PyTree.\n",
    "\n",
    "For example, suppose you want to square each element of the arrays. You can\n",
    "then apply this using the `tree_map` function from the `jax.tree_util` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf881fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([1., 4., 9.], dtype=float32), Array([16., 25., 36.], dtype=float32), Array([49., 64., 81.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jtu.tree_map(lambda x: x**2, tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50eb4c8",
   "metadata": {},
   "source": [
    "In this example, the PyTree makes it easy to apply a function to each leaf of\n",
    "a complex data structure, without having to manually traverse the data\n",
    "structure and handle each leaf individually. JAX PyTrees, therefore, are a\n",
    "powerful tool that can simplify many tasks in machine learning and scientific\n",
    "computing. As such, most JAX functions operate over _PyTrees of JAX arrays_.\n",
    "For instance, `jax.lax.scan`, accepts as input and produces as output a\n",
    "PyTree of JAX arrays.\n",
    "\n",
    "Another key advantages of using JAX PyTrees is that they are designed to work\n",
    "efficiently with JAX's automatic differentiation and compilation features. For\n",
    "example, suppose you have a function that takes a PyTree as input and returns\n",
    "a scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2eec409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(285., dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_squares(x):\n",
    "    return jnp.sum(x[0] ** 2 + x[1] ** 2 + x[2] ** 2)\n",
    "\n",
    "sum_squares(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3498bc",
   "metadata": {},
   "source": [
    "You can use JAX's `grad` function to automatically compute the gradient of\n",
    "this function with respect to the input PyTree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5fce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([2., 4., 6.], dtype=float32), Array([ 8., 10., 12.], dtype=float32), Array([14., 16., 18.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "gradient = jax.grad(sum_squares)(tree)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e74591",
   "metadata": {},
   "source": [
    "This computes the gradient of the `sum_squares` function with respect to the\n",
    "input PyTree, and returns a new PyTree with the same shape and structure.\n",
    "\n",
    "JAX PyTrees are also designed to be highly extensible, where custom types can be readily registered through a global registry with the\n",
    "values of such traversed recursively (i.e., as a tree!). This means we can\n",
    "define our own custom data structures and use them as PyTrees. This is the\n",
    "functionality that we exploit, whereby we construct all Gaussian process\n",
    "models via a tree-structure through our `Module` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e0bfc",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "Core idea is represent all model objects via\n",
    "an immutable PyTree.\n",
    "- leaves of the PyTree represent the parameters\n",
    "that are to be trained\n",
    "- describe their domain and trainable status as\n",
    "`dataclass` metadata.\n",
    "\n",
    "- For our RBF kernel we have two parameters; the lengthscale and the variance.\n",
    "Both of these have positive domains, and by default we want to train both of\n",
    "these parameters. \n",
    "- To encode this we use a `param_field`, where we can define\n",
    "the domain of both parameters via a `Softplus` bijector (that restricts them\n",
    "to the positive domain), and set their trainable status to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb640e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "from gpjax.base import Module, param_field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RBF(Module):\n",
    "    lengthscale: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n",
    "    variance: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n",
    "\n",
    "    def __call__(self, x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "        return self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfeca2e",
   "metadata": {},
   "source": [
    "Here `param_field` is just a special type of `dataclasses.field`. As such the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54efb194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x1037b8580>,default_factory=<function param_field.<locals>.<lambda> at 0x2c8a4d3f0>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'bijector': <tfp.bijectors.Identity 'identity' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?>, 'trainable': False, 'pytree_node': True}),kw_only=<dataclasses._MISSING_TYPE object at 0x1037b8580>,_field_type=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_field(1.0, bijector= tfb.Identity(), trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba9759",
   "metadata": {},
   "source": [
    "is equivalent to the following `dataclasses.field`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d43a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field(name=None,type=None,default=1.0,default_factory=<dataclasses._MISSING_TYPE object at 0x1037b8580>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'trainable': False, 'bijector': <tfp.bijectors.Identity 'identity' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?>}),kw_only=<dataclasses._MISSING_TYPE object at 0x1037b8580>,_field_type=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field(default=1.0, metadata={\"trainable\": False, \"bijector\": tfb.Identity()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4d838",
   "metadata": {},
   "source": [
    "By default unmarked leaf attributes default to an `Identity` bijector and\n",
    "trainablility set to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36fa02",
   "metadata": {},
   "source": [
    "Critically we can now take the gradient of the kernel with respect to its parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "415e15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBF()\n",
    "\n",
    "try:\n",
    "    jax.grad(lambda kern: kern(1.0, 2.0))(kernel)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f6da",
   "metadata": {},
   "source": [
    "# Efficient GPs\n",
    "\n",
    "The white noise kernel has covariance,\n",
    "$$\n",
    "    k(x, y) = \\sigma^2 \\delta(x-y)\n",
    "$$\n",
    "\n",
    "We can code this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a3914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WhiteNoise(Module):\n",
    "    variance: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n",
    "\n",
    "    def __call__(self, x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "        K = jnp.all(jnp.equal(x, y)) * self.variance\n",
    "        return K.squeeze()\n",
    "\n",
    "kernel = WhiteNoise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8112c",
   "metadata": {},
   "source": [
    "Recall the costly covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15a3f9",
   "metadata": {},
   "source": [
    "$$k(\\mathbf{x}, \\mathbf{x}) = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) & \\cdots & k(\\mathbf{x}_1, \\mathbf{x}_n) \\\\ \\vdots & \\ddots & \\vdots \\\\ k(\\mathbf{x}_n, \\mathbf{x}_1) & \\cdots & k(\\mathbf{x}_n, \\mathbf{x}_n) \\end{bmatrix}$$\n",
    "\n",
    "Lets compute this for some data using `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113d3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap\n",
    "\n",
    "# datapoints:\n",
    "x = jnp.linspace(-3., 3., 50)\n",
    "\n",
    "\n",
    "# function to compute the gram matrix:\n",
    "def gram(kernel: WhiteNoise, x:  jax.Array) -> jax.Array:\n",
    "    return vmap(lambda xi: vmap(lambda yi: kernel(xi, yi))(x))(x)\n",
    "\n",
    "\n",
    "# compute the gram matrix:\n",
    "print(gram(kernel, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384fde5",
   "metadata": {},
   "source": [
    "Notice that the covariance is zero everywhere except when $x=y$. This means the Gram matrix is sparse!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b324d",
   "metadata": {},
   "source": [
    "### Compute engine:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae5daa",
   "metadata": {},
   "source": [
    "We introduce computation engines for computing the Linear Operators instead of the dense Gram matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3369922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cola\n",
    "from cola.ops import Diagonal, Dense, LinearOperator\n",
    "\n",
    "@dataclass\n",
    "class AbstractComputeEngine:\n",
    "    def gram(self, kernel, x) -> LinearOperator:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cd1cd",
   "metadata": {},
   "source": [
    "Naive computation like before can be represented as a `Dense` linear operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2048fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default:\n",
    "@dataclass\n",
    "class DenseComputeEngine(AbstractComputeEngine):\n",
    "    def gram(self, kernel, x) -> Dense:\n",
    "        return Dense(vmap(lambda xi: vmap(lambda yi: kernel(xi, yi))(x))(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d9a45",
   "metadata": {},
   "source": [
    "But, given the structure, we can save memory and compute time by using a `Diagonal` linear operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f42e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured:\n",
    "@dataclass\n",
    "class DiagonalComputeEngine(AbstractComputeEngine):\n",
    "    def gram(self, kernel, x) -> Diagonal:\n",
    "        return Diagonal(vmap(lambda xi: kernel(xi, xi))(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a542e43",
   "metadata": {},
   "source": [
    "We can set the default compute enegine of the kernel to be `Diagonal`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9065f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearOperator: diag([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.])\n",
      "LinearOperator as dense matrix: [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class WhiteNoise(Module):\n",
    "    variance: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n",
    "    compute_engine: AbstractComputeEngine = DiagonalComputeEngine()\n",
    "\n",
    "    def __call__(self, x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "        K = jnp.all(jnp.equal(x, y)) * self.variance\n",
    "        return K.squeeze()\n",
    "\n",
    "    def gram(self, x: jax.Array) -> jax.Array:\n",
    "        return self.compute_engine.gram(self, x)\n",
    "\n",
    "\n",
    "kernel = WhiteNoise()\n",
    "Kxx = kernel.gram(x)\n",
    "print(f\"LinearOperator: {Kxx}\")\n",
    "print(f\"LinearOperator as dense matrix: {Kxx.to_dense()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf004e0f",
   "metadata": {},
   "source": [
    "The compute engine can be changed for approximations e.g., Random Fourier Features!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
